<div align="center">

# ğŸ’Š GraphTransDTI: Drug-Target Interaction Prediction

### A State-of-the-Art Deep Learning Framework for *In Silico* Drug Discovery

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange?style=for-the-badge&logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)]()

<p align="center">
  <em>"Unlocking the molecular language of life through Geometric Deep Learning and Attention Mechanisms."</em>
</p>

</div>

---

## ğŸ“‹ Executive Summary

**GraphTransDTI** is an advanced end-to-end learning framework designed to predict the binding affinity between drug molecules and protein targets. By treating molecules as graphs and proteins as sequences, we leverage the power of **Graph Transformers** and **Bi-directional LSTMs** to capture intricate structural and sequential patterns.

The core innovation lies in our **Cross-Attention Interface**, which mimics the biological docking process computationally, allowing the model to focus on specific binding sites.

### ğŸš€ Key Objectives
- **Accuracy:** Surpass state-of-the-art (SOTA) baselines (DeepDTA, GraphDTA) by at least **10%** in RMSE.
- **Robustness:** Generalize well across diverse datasets (DAVIS, KIBA).
- **Efficiency:** Optimize inference time for high-throughput screening.

---

## ğŸ§® Methodology & Mathematical Formulation

### 1. Drug Encoding (Graph Transformer)
We represent drugs as molecular graphs $G = (V, E)$. To capture global structural information, we utilize Multi-Head Self-Attention:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where $Q, K, V$ are feature matrices derived from atom nodes.

### 2. Protein Encoding (Sequence Modeling)
Target proteins are processed as amino acid sequences using a hybrid CNN-BiLSTM architecture to capture local motifs and long-range dependencies:

$$
h_t = \text{BiLSTM}(\text{CNN}(x_{protein}))
$$

### 3. Interaction Learning (Cross-Attention)
To model the binding affinity, we compute the interaction matrix between Drug features ($H_d$) and Protein features ($H_p$):

$$
\text{Interaction} = \text{MultiHead}(Q=H_d, K=H_p, V=H_p)
$$

Finally, the binding affinity score $\hat{y}$ is predicted via a Multi-Layer Perceptron (MLP).

---

## ğŸ—ï¸ System Architecture

The detailed workflow of GraphTransDTI is illustrated below:

```mermaid
graph LR
    subgraph Input
    D[SMILES Drug]
    P[Protein Sequence]
    end

    subgraph Encoders
    D -->|RDKit| GE(Graph Transformer)
    P -->|Tokenize| PE(CNN + BiLSTM)
    end

    subgraph Fusion
    GE --> CA{Cross-Attention\nFusion Layer}
    PE --> CA
    end

    subgraph Output
    CA --> FCN[Fully Connected Network]
    FCN --> RES((Binding Affinity\npKd / KIBA))
    end
    
    style CA fill:#f96,stroke:#333,stroke-width:2px
    style RES fill:#9f9,stroke:#333,stroke-width:2px

    ğŸ“Š Dataset StatisticsWe benchmark our model on two standard datasets in the field.Dataset# Drugs# Proteins# InteractionsSparsityLabel UnitDAVIS6844230,056100% (Dense)$K_d$ (dissociation constant)KIBA2,111229118,25424.4% (Sparse)KIBA Score
    ğŸ“ˆ Experimental ResultsComparison with baseline methods on the DAVIS Test Set:ModelRMSE (Lower is better) â†“CI (Higher is better) â†‘MSE â†“DeepDTA (CNN-based)0.2610.8780.261GraphDTA (GNN-based)0.2290.8920.229MolTrans (Transformer)0.2200.8980.220GraphTransDTI (Ours)0.2100.9050.210
    ğŸ› ï¸ Installation & Usage
Prerequisites
Python 3.8+

PyTorch 2.0+

CUDA (Recommended for GPU acceleration)

1. Setup
Bash

# Clone the repo
git clone [https://github.com/WinKy1-stack/GraphTransDTI.git](https://github.com/WinKy1-stack/GraphTransDTI.git)
cd GraphTransDTI

# Install dependencies
pip install -r requirements.txt
2. Training
To train the model from scratch on the DAVIS dataset:

Bash

python src/train.py --dataset davis --batch_size 64 --epochs 100 --lr 0.0005
3. Evaluation
To test a pre-trained model:

Bash

python evaluate.py --checkpoint checkpoints/best_model.pt --dataset davis
ğŸ—‚ï¸ Project Structure
GraphTransDTI/
â”œâ”€â”€ data/                   # Dataset storage
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/             # Model definitions (GNN, LSTM, Attention)
â”‚   â”œâ”€â”€ dataloader/         # Data processing pipelines
â”‚   â”œâ”€â”€ utils/              # Helper functions & Metrics
â”‚   â””â”€â”€ train.py            # Main training script
â”œâ”€â”€ notebooks/              # Jupyter Notebooks for EDA
â”œâ”€â”€ configs/                # Configuration files (YAML)
â”œâ”€â”€ results/                # Saved models and logs
â””â”€â”€ README.md               # Project Documentation
ğŸ›¤ï¸ Roadmap
[x] Phase 1: Implement Baseline Encoders (GCN, CNN).

[x] Phase 2: Develop Graph Transformer & Cross-Attention.

[x] Phase 3: Benchmark on DAVIS & KIBA datasets.

[ ] Phase 4: Integrate 3D Protein Structure (AlphaFold).

[ ] Phase 5: Develop Web Interface (Streamlit) for real-time prediction.

ğŸ“š References
Ã–ztÃ¼rk, H., et al. "DeepDTA: deep drugâ€“target binding affinity prediction." Bioinformatics (2018).

Nguyen, T., et al. "GraphDTA: predicting drugâ€“target binding affinity with graph neural networks." Bioinformatics (2021).

Vaswani, A., et al. "Attention is all you need." NIPS (2017).

ğŸ‘¤ Author
SÆ¡n (WinKy1-stack)

Role: AI Engineer / Researcher

Interests: Deep Learning, Bioinformatics, Computer Vision.

GitHub: WinKy1-stack

Contact: 0888873104t@gmail.com

<div align="center">


<img src="https://www.google.com/search?q=https://img.shields.io/badge/Built%2520with-Love%2520%2526%2520Coffee-ff69b4%3Fstyle%3Dfor-the-badge" /> </div>
