# GraphTransDTI: Drug-Target Interaction Prediction

## á»¨ng dá»¥ng mÃ´ hÃ¬nh dá»±a trÃªn Ä‘á»“ thá»‹ cho khÃ¡m phÃ¡ vÃ  dá»± Ä‘oÃ¡n thuá»‘c trong y dÆ°á»£c

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-red.svg)](https://pytorch.org/)
[![PyG](https://img.shields.io/badge/PyG-2.6.1-orange.svg)](https://pytorch-geometric.readthedocs.io/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Tá»•ng quan

**GraphTransDTI** lÃ  mÃ´ hÃ¬nh deep learning cho **dá»± Ä‘oÃ¡n tÆ°Æ¡ng tÃ¡c thuá»‘c-protein (Drug-Target Interaction - DTI)**, káº¿t há»£p:

- **Graph Transformer** cho phÃ¢n tá»­ thuá»‘c (SMILES â†’ Ä‘á»“ thá»‹ phÃ¢n tá»­)
- **CNN + BiLSTM** cho chuá»—i protein (amino acid sequence)
- **Cross-Attention** há»c tÆ°Æ¡ng tÃ¡c giá»¯a thuá»‘c vÃ  protein
- **MLP Predictor** dá»± Ä‘oÃ¡n binding affinity

### ğŸ¯ Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

âœ… **RMSE giáº£m 8.08%** so vá»›i baseline DeepDTA (0.461 vs 0.502)  
âœ… **Pearson correlation tÄƒng 0.012** (0.835 vs 0.823)  
âœ… **Concordance Index Ä‘áº¡t 0.840** (vÆ°á»£t má»¥c tiÃªu > 0.83)  

### ğŸ† Æ¯u Ä‘iá»ƒm

| KhÃ­a cáº¡nh | GraphTransDTI | Baseline (DeepDTA/GraphDTA) |
|-----------|---------------|----------|
| **Drug Encoder** | Graph Transformer (global attention) | GCN/GAT (local aggregation) |
| **Protein Encoder** | CNN + BiLSTM (bidirectional) | CNN hoáº·c LSTM |
| **Fusion** | Cross-Attention (8 heads) | Concatenation + FC |
| **Parameters** | 2.06M | ~1.5M |
| **Training Time** | 5-6h (RTX 3050 4GB) | 4-5h |

---

## ğŸ—‚ï¸ Cáº¥u trÃºc dá»± Ã¡n

```text
GraphTransDTI/
â”‚
â”œâ”€â”€ data/                              # Datasets
â”‚   â”œâ”€â”€ kiba/                          # KIBA dataset (training)
â”‚   â”‚   â”œâ”€â”€ ligands_can.txt            # 2,111 SMILES strings
â”‚   â”‚   â”œâ”€â”€ proteins.txt               # 229 protein sequences
â”‚   â”‚   â””â”€â”€ Y                          # Affinity matrix (pickle)
â”‚   â”œâ”€â”€ davis/                         # DAVIS dataset (testing)
â”‚   â””â”€â”€ DATA_DOWNLOAD_GUIDE.md         # Dataset instructions
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ models/                        # Model components
â”‚   â”‚   â”œâ”€â”€ graph_transformer.py       # Drug encoder (Graph Transformer)
â”‚   â”‚   â”œâ”€â”€ protein_encoder.py         # Protein encoder (CNN+BiLSTM)
â”‚   â”‚   â”œâ”€â”€ cross_attention.py         # Cross-attention fusion
â”‚   â”‚   â””â”€â”€ graphtransdti.py           # Complete model
â”‚   â”‚
â”‚   â”œâ”€â”€ dataloader/                    # Data processing
â”‚   â”‚   â”œâ”€â”€ featurizer.py              # Drug-protein featurization
â”‚   â”‚   â”œâ”€â”€ kiba_loader.py             # KIBA dataset loader
â”‚   â”‚   â””â”€â”€ davis_loader.py            # DAVIS dataset loader
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                         # Utilities
â”‚   â”‚   â”œâ”€â”€ metrics.py                 # Evaluation metrics (RMSE, Pearson, CI)
â”‚   â”‚   â”œâ”€â”€ seed.py                    # Reproducibility
â”‚   â”‚   â”œâ”€â”€ smiles_to_graph.py         # SMILES â†’ Graph conversion (RDKit)
â”‚   â”‚   â””â”€â”€ visualizer.py              # Plotting functions
â”‚   â”‚
â”‚   â”œâ”€â”€ train.py                       # Main training script
â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation script
â”‚   â”œâ”€â”€ test_davis.py                  # DAVIS testing
â”‚   â””â”€â”€ plot_results.py                # Result visualization
â”‚
â”œâ”€â”€ checkpoints/                       # Saved models
â”‚   â”œâ”€â”€ GraphTransDTI_KIBA_best.pt     # Best model (epoch 94)
â”‚   â””â”€â”€ GraphTransDTI_KIBA_history.pkl # Training history
â”‚
â”œâ”€â”€ results/                           # Experimental results
â”‚   â”œâ”€â”€ figures/                       # Training/evaluation plots
â”‚   â”œâ”€â”€ davis_normalized/              # DAVIS test results
â”‚   â”œâ”€â”€ results_summary.json           # Metrics (JSON)
â”‚   â””â”€â”€ COMPREHENSIVE_RESULTS.txt      # Full report
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ BAO_CAO_KHOA_HOC.md           # Scientific report (Vietnamese)
â”‚   â”œâ”€â”€ DATASETS_USAGE_STRATEGY.md    # Dataset strategy
â”‚   â”œâ”€â”€ MODEL_ARCHITECTURE.md         # Architecture details
â”‚   â””â”€â”€ RESULTS_SUMMARY.md            # Results analysis
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks (optional)
â”‚
â”œâ”€â”€ config.yaml                        # Hyperparameters
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ test_davis_normalized.py           # DAVIS normalization test
â”œâ”€â”€ LICENSE                            # MIT License
â””â”€â”€ README.md                          # This file
```

---

## âš™ï¸ CÃ i Ä‘áº·t

### 1. Clone repository

```bash
git clone https://github.com/WinKy1-stack/GRAPTRAINDTI.git
cd GRAPTRAINDTI
```

### 2. Táº¡o mÃ´i trÆ°á»ng áº£o

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

**Requirements chÃ­nh:**
- Python 3.8+
- PyTorch 2.5.1
- PyTorch Geometric 2.6.1
- RDKit 2024.3.6
- NumPy, Pandas, Matplotlib, Seaborn

**LÆ°u Ã½ cho Windows + CUDA:**

```bash
# Install PyTorch with CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install PyG
pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.5.0+cu118.html
```

### 4. Táº£i dataset

Xem hÆ°á»›ng dáº«n chi tiáº¿t táº¡i: `data/DATA_DOWNLOAD_GUIDE.md`

**Quick start:**

```bash
# KIBA (tá»« DeepDTA repository)
# Download vÃ  extract vÃ o data/kiba/
# Files cáº§n: ligands_can.txt, proteins.txt, Y (pickle file)

# DAVIS
# Download vÃ  extract vÃ o data/davis/
# Files cáº§n: ligands_can.txt, proteins.txt, Y (pickle file)
```

**Hoáº·c sá»­ dá»¥ng dataset Ä‘Ã£ cÃ³:**
- CÃ¡c file dataset Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ sáºµn trong `data/kiba/` vÃ  `data/davis/`

---

## ğŸš€ Sá»­ dá»¥ng

### 1. Training KIBA

```bash
cd src
python train.py
```

**Configuration** (Ä‘Ã£ training vá»›i config nÃ y):

```yaml
# config.yaml
model:
  drug_encoder:
    hidden_dim: 128
    num_layers: 4
    num_heads: 8
    dropout: 0.2
  protein_encoder:
    embedding_dim: 128
    lstm_hidden_dim: 128
    lstm_num_layers: 2
    cnn_filters: [4, 6, 8]
    cnn_channels: 128
  fusion:
    num_heads: 8
  predictor:
    hidden_dims: [256, 128, 64]
    dropout: 0.2

training:
  batch_size: 64
  learning_rate: 0.0001
  epochs: 100
  early_stopping_patience: 15
```

**Training results** Ä‘Æ°á»£c lÆ°u táº¡i:
- `checkpoints/GraphTransDTI_KIBA_best.pt` - Best model (epoch 94)
- `results/training_progress/` - Training curves
- `results/figures/` - Evaluation plots

### 2. Cross-dataset Test (DAVIS)

```bash
python test_davis_normalized.py
```

Script nÃ y sáº½:
- Load model Ä‘Ã£ train trÃªn KIBA
- Normalize DAVIS dataset (Kd â†’ pKd â†’ KIBA scale)
- Evaluate vÃ  táº¡o visualizations

### 3. Visualization

```python
# Plot training history
from src.visualize_results import plot_training_results

plot_training_results(
    history_path='checkpoints/GraphTransDTI_KIBA_history.pkl',
    save_dir='results/figures'
)
```

Results bao gá»“m:
- Training/Validation loss curves
- RMSE, Pearson, CI curves
- Scatter plots (Predicted vs Actual)
- Distribution plots

---

## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m

### KIBA Dataset - Training & Evaluation

**Dataset thÃ´ng tin:**
- Total pairs: 118,254 (2,111 drugs Ã— 229 proteins)
- Train/Val/Test: 94,603 / 11,825 / 11,826 (80%/10%/10%)
- Training: 100 epochs, best at epoch 94

**Káº¿t quáº£ so sÃ¡nh:**

| Model | RMSE â†“ | Pearson r â†‘ | CI â†‘ | Year |
|-------|--------|-------------|------|------|
| DeepDTA | 0.502 | 0.823 | 0.831 | 2018 |
| WideDTA | 0.498 | 0.825 | 0.833 | 2019 |
| GraphDTA | 0.495 | 0.827 | 0.835 | 2021 |
| GAT-DTI | 0.485 | 0.831 | 0.838 | 2022 |
| **GraphTransDTI (Ours)** | **0.461** | **0.835** | **0.840** | **2025** |

**Cáº£i tiáº¿n:**
- ğŸ¯ RMSE giáº£m **8.08%** so vá»›i DeepDTA (0.502 â†’ 0.461)
- ğŸ“ˆ Pearson r tÄƒng tá»« 0.823 â†’ 0.835
- â­ CI tÄƒng tá»« 0.831 â†’ 0.840

### DAVIS Dataset - Cross-dataset Evaluation

**Normalized test (Kd â†’ pKd â†’ KIBA scale):**

| Metric | Value | Note |
|--------|-------|------|
| RMSE | 10.91 | Cross-dataset (KIBA train â†’ DAVIS test) |
| Pearson r | 0.406 | KhÃ¡c scale, khÃ¡c phÃ¢n bá»‘ so vá»›i KIBA |
| Spearman Ï | 0.352 | Ranking correlation |
| CI | 0.687 | Good ranking ability |

**PhÃ¢n tÃ­ch:**
- âœ… Model generalize Ä‘Æ°á»£c sang dataset má»›i (DAVIS)
- âœ… CI = 0.687 chá»©ng tá» kháº£ nÄƒng xáº¿p háº¡ng tá»‘t (quan trá»ng trong drug discovery)
- âš ï¸ RMSE cao hÆ¡n do khÃ¡c phÃ¢n bá»‘ (KIBA: 0-15, DAVIS: pKd scale)

---

## ğŸ”¬ Kiáº¿n trÃºc mÃ´ hÃ¬nh

```text
Input:
  Drug: SMILES string â†’ RDKit â†’ Molecular Graph (nodes: atoms, edges: bonds)
  Protein: Amino acid sequence â†’ Embedding â†’ [batch, seq_len, 128]

Drug Encoder (Graph Transformer):
  - 4 Transformer layers
  - 8 attention heads per layer
  - Hidden dim: 128
  - Global attention trÃªn toÃ n bá»™ Ä‘á»“ thá»‹ phÃ¢n tá»­
  Output: [batch, 128]

Protein Encoder (CNN + BiLSTM):
  - Embedding layer: vocab_size=26 â†’ dim=128
  - 3 CNN filters: [4, 6, 8] with 128 channels each
  - BiLSTM: 2 layers, hidden_dim=128
  - Max pooling over sequence length
  Output: [batch, 128]

Fusion (Cross-Attention):
  - Multi-head attention: 8 heads
  - Drug attends to Protein context
  - Protein attends to Drug context
  - Concatenation: [batch, 256]
  Output: [batch, 256]

Predictor (MLP):
  Linear(256 â†’ 128) â†’ ReLU â†’ Dropout(0.2)
  â†’ Linear(128 â†’ 64) â†’ ReLU â†’ Dropout(0.2)
  â†’ Linear(64 â†’ 1)
  Output: Binding affinity (scalar)
```

### Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t

- **Total parameters**: 2,058,049 (~2.06M)
- **Drug Encoder**: 789,760 params (Graph Transformer)
- **Protein Encoder**: 855,808 params (CNN + BiLSTM)
- **Cross-Attention**: 131,712 params
- **Predictor**: 280,769 params
- **Training time**: ~5-6 hours (KIBA, 100 epochs, RTX 3050 4GB)
- **Inference**: ~40-50 predictions/second (GPU)
- **Memory**: ~3.5GB GPU RAM during training

---

## ğŸ“š Datasets

### KIBA (Kinase Inhibitor BioActivity) - Training Set

- **Drugs**: 2,111 kinase inhibitors
- **Proteins**: 229 kinases  
- **Interactions**: 118,254 drug-protein pairs
- **Affinity**: KIBA score (normalized, 0-15 range)
- **Usage**: Training + Validation + Test (80/10/10 split)
- **Source**: [Davis et al. 2011](https://www.nature.com/articles/nbt.1990)

### DAVIS - Cross-dataset Test

- **Drugs**: 68 kinase inhibitors
- **Proteins**: 442 kinases
- **Interactions**: 30,056 drug-protein pairs  
- **Affinity**: Kd (dissociation constant, 0.02-10,000 nM)
- **Usage**: Cross-dataset generalization test
- **Normalization**: Kd â†’ pKd â†’ KIBA scale (for evaluation)

### BindingDB - Future Work

- **Interactions**: > 1,000,000 drug-target pairs
- **Usage**: Pre-training Ä‘á»ƒ improve generalization
- **Strategy**: Pre-train on BindingDB â†’ fine-tune on KIBA

---

## ğŸ› ï¸ PhÃ¡t triá»ƒn & TÃ­nh nÄƒng

### âœ… ÄÃ£ hoÃ n thÃ nh

- [x] **Graph Transformer** cho drug encoding vá»›i global attention
- [x] **CNN + BiLSTM** cho protein sequence encoding
- [x] **Cross-Attention** fusion mechanism (8 heads)
- [x] **Training pipeline** vá»›i early stopping, learning rate scheduling
- [x] **Comprehensive evaluation**: RMSE, Pearson r, Spearman Ï, CI
- [x] **Cross-dataset test** KIBA â†’ DAVIS vá»›i normalization
- [x] **Visualization**: 8 training curves + evaluation plots
- [x] **Complete documentation**: Scientific report, usage guide

### ğŸš€ HÆ°á»›ng cáº£i tiáº¿n (Future Work)

- [ ] **3D Structure Integration**: Sá»­ dá»¥ng AlphaFold protein structure
- [ ] **Pre-training**: Large-scale pre-train trÃªn BindingDB â†’ fine-tune KIBA
- [ ] **Multi-task Learning**: Dá»± Ä‘oÃ¡n binding affinity + binding site + Ki/Kd/IC50
- [ ] **Interpretability**: 
  - Attention visualization (drug-protein interaction heatmap)
  - GradCAM for important atoms/residues
  - SHAP values for feature importance
- [ ] **Ablation Study**: Äo contribution cá»§a tá»«ng component
- [ ] **Web Demo**: Flask/Streamlit app cho prediction interface
- [ ] **Ensemble**: Combine multiple models Ä‘á»ƒ improve robustness

---

## ğŸ“Š Results & Visualizations

Project bao gá»“m comprehensive results:

### Training Results
- `results/figures/` - 8 training/evaluation plots:
  - Training & Validation Loss curves
  - RMSE progression  
  - Pearson correlation progression
  - Concordance Index progression
  - Prediction vs Actual scatter plots (train/val/test)
  - Distribution comparison plots

### Performance Metrics
- `results/results_summary.json` - JSON format metrics
- `results/COMPREHENSIVE_RESULTS.txt` - Human-readable report
- `checkpoints/GraphTransDTI_KIBA_best.pt` - Best model (epoch 94)
- `checkpoints/GraphTransDTI_KIBA_history.pkl` - Full training history

### Cross-dataset Test
- `results/davis_normalized/` - DAVIS evaluation results
  - Normalization analysis (Kd â†’ pKd â†’ KIBA)
  - Evaluation metrics vÃ  plots
  - Comparison vá»›i KIBA results

---

## ğŸ“– TÃ i liá»‡u tham kháº£o

### Key Papers

1. **Davis et al. (2011)** - "Comprehensive analysis of kinase inhibitor selectivity" - *Nature Biotechnology*
2. **Ã–ztÃ¼rk et al. (2018)** - "DeepDTA: deep drugâ€“target binding affinity prediction" - *Bioinformatics*
3. **Nguyen et al. (2021)** - "GraphDTA: predicting drugâ€“target binding affinity with graph neural networks" - *Bioinformatics*
4. **Vaswani et al. (2017)** - "Attention is All You Need" - *NeurIPS*
5. **Ying et al. (2021)** - "Do Transformers Really Perform Bad for Graph Representation?" - *NeurIPS*

### Libraries & Tools

- [PyTorch](https://pytorch.org/) - Deep learning framework
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) - Graph neural networks
- [RDKit](https://www.rdkit.org/) - Cheminformatics and molecular featurization
- [DeepDTA Repository](https://github.com/hkmztrk/DeepDTA) - Dataset preprocessing

---

## ğŸ‘¨â€ğŸ’» TÃ¡c giáº£

**Äá»“ Ã¡n tá»‘t nghiá»‡p**: á»¨ng dá»¥ng mÃ´ hÃ¬nh dá»±a trÃªn Ä‘á»“ thá»‹ cho khÃ¡m phÃ¡ vÃ  dá»± Ä‘oÃ¡n thuá»‘c trong y dÆ°á»£c

- **Sinh viÃªn**: Nguyá»…n Thá»‹ NhÆ°
- **MSSV**: [MSSV cá»§a báº¡n]
- **Lá»›p**: [Lá»›p cá»§a báº¡n]
- **TrÆ°á»ng**: Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin
- **Giáº£ng viÃªn hÆ°á»›ng dáº«n**: [TÃªn GVHD]
- **NÄƒm**: 2024-2025

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- **DeepDTA team** for dataset preprocessing and baseline implementation
- **PyTorch Geometric** community for graph neural network library
- **RDKit** developers for molecular featurization tools
- **Attention is All You Need** (Vaswani et al.) for Transformer architecture

---

## ğŸ“§ LiÃªn há»‡

- **GitHub**: [WinKy1-stack](https://github.com/WinKy1-stack)
- **Repository**: [GRAPTRAINDTI](https://github.com/WinKy1-stack/GRAPTRAINDTI)
- **Email**: [ThÃªm email náº¿u muá»‘n cÃ´ng khai]

---

## ğŸ“– Citation

Náº¿u sá»­ dá»¥ng code nÃ y trong nghiÃªn cá»©u, vui lÃ²ng cite:

```bibtex
@misc{graphtransdti2025,
  author = {Nguyá»…n Thá»‹ NhÆ°},
  title = {GraphTransDTI: Drug-Target Interaction Prediction using Graph Transformers},
  year = {2025},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/WinKy1-stack/GRAPTRAINDTI}}
}
```

---

**Cáº­p nháº­t láº§n cuá»‘i**: 19/11/2025
