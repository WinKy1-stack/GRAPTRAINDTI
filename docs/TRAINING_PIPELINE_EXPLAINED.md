# ğŸ“š GRAPHTRANSDTI TRAINING PIPELINE - GIáº¢I THÃCH CHI TIáº¾T

## âœ… GPU SETUP HOÃ€N Táº¤T
```
GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4GB VRAM)
CUDA: 12.8 (Driver 572.83)
PyTorch: 2.5.1+cu121
Training Speed: Æ¯á»›c tÃ­nh 2-3 giá» cho 100 epochs (nhanh hÆ¡n CPU 10-15x)
```

---

## ğŸ“ CÃC FILE Cáº¦N THIáº¾T Äá»‚ TRAIN

### 1ï¸âƒ£ **DATA FILES** (ÄÃ£ cÃ³ Ä‘á»§)
```
data/kiba/
â”œâ”€â”€ ligands_can.txt      # 2,111 SMILES (JSON format)
â”œâ”€â”€ proteins.txt         # 229 protein sequences (JSON format)
â””â”€â”€ Y                    # Affinity matrix (2111Ã—229)

data/davis/
â”œâ”€â”€ ligands_can.txt      # 68 SMILES
â”œâ”€â”€ proteins.txt         # 442 sequences
â””â”€â”€ Y                    # Affinity matrix (68Ã—442)
```

**Vai trÃ²**: Dá»¯ liá»‡u Ä‘áº§u vÃ o - 118K drug-protein pairs cho training

---

### 2ï¸âƒ£ **CONFIG FILE**
```
config.yaml
```

**Vai trÃ²**: Hyperparameters - Ä‘iá»u khiá»ƒn toÃ n bá»™ quÃ¡ trÃ¬nh training

**CÃ¡c tham sá»‘ quan trá»ng**:
```yaml
experiment:
  name: "GraphTransDTI_KIBA"
  device: "cuda"                    # âœ… Sá»­ dá»¥ng GPU
  checkpoint_dir: "./checkpoints"   # LÆ°u model tá»‘t nháº¥t

training:
  batch_size: 64                    # 64 pairs/batch
  num_epochs: 100                   # Tá»‘i Ä‘a 100 epochs
  learning_rate: 0.0001
  early_stopping_patience: 15       # âœ… Stop náº¿u khÃ´ng improve 15 epochs

model:
  drug_encoder:
    hidden_dim: 128
    num_layers: 4                   # 4 Graph Transformer layers
    num_heads: 8                    # Multi-head attention
  
  protein_encoder:
    cnn_filters: [32, 64, 96]       # CNN cho protein motifs
    lstm_num_layers: 2              # BiLSTM cho context
  
  cross_attention:
    num_heads: 8                    # Drug-protein interaction
```

---

### 3ï¸âƒ£ **DATALOADER FILES**

#### A. `src/dataloader/featurizer.py`
**Logic**: Chuyá»ƒn Ä‘á»•i raw data â†’ model-ready tensors

```python
SMILES string â†’ RDKit Molecule â†’ Graph
  â”œâ”€â”€ Node features: 78-dim (atom type, degree, aromatic, ...)
  â”œâ”€â”€ Edge features: 12-dim (bond type, conjugated, ring, ...)
  â””â”€â”€ PyG Data object

Protein sequence â†’ Token indices
  â”œâ”€â”€ 26 tokens: 20 amino acids + 5 special (PAD, UNK, ...)
  â”œâ”€â”€ Padding/Truncate to 1000 length
  â””â”€â”€ Tensor [1000]
```

**Input Example**:
```python
SMILES: "CCO"                    # Ethanol
Protein: "MKVLWAALL..."          # 500 amino acids
Label: 12.5                      # KIBA score
```

**Output**:
```python
{
  'drug': PyG Data(x=[3, 78], edge_index=[2, 4], edge_attr=[4, 12]),
  'protein': Tensor[1000],       # Padded to 1000
  'label': Tensor[1]
}
```

#### B. `src/dataloader/kiba_loader.py`
**Logic**: Load KIBA dataset vÃ  split train/val/test

```python
1. Load JSON files
   ligands_dict = json.load('ligands_can.txt')   # {"CHEMBL123": "CCO", ...}
   proteins_dict = json.load('proteins.txt')
   affinity_matrix = pickle.load('Y')             # Shape (2111, 229)

2. Create pairs
   for i in range(2111):                          # Drugs
       for j in range(229):                       # Proteins
           if not np.isnan(affinity_matrix[i,j]):
               pairs.append((smiles[i], proteins[j], affinity_matrix[i,j]))
   
   â†’ 118,254 valid pairs

3. Split dataset
   Random shuffle with seed=42
   80% train:  94,603 pairs
   10% val:    11,825 pairs
   10% test:   11,826 pairs

4. Create DataLoader
   Batch size: 64
   Collate function: collate_dti_batch()
```

#### C. `src/dataloader/davis_loader.py`
**Logic**: TÆ°Æ¡ng tá»± KIBA, dÃ¹ng cho generalization testing

---

### 4ï¸âƒ£ **MODEL FILES**

#### A. `src/models/graph_transformer.py`
**Logic**: Encode drug molecules

```python
Graph Transformer Layer:
  Input: Node features [num_atoms, 78], Edges [2, num_bonds]
  
  Step 1: Multi-Head Attention
    Q, K, V = Linear(x)
    Attention(Q, K, V) = softmax(QK^T/âˆšd) Â· V
    â†’ Atoms attend to each other (learn molecular structure)
  
  Step 2: Feedforward Network
    FFN(x) = ReLU(Linear(x)) â†’ Linear
  
  Step 3: Residual + LayerNorm
    Output = LayerNorm(x + Attention(x))
            + LayerNorm(x + FFN(x))

GraphTransformerEncoder:
  Embedding: [78] â†’ [128]
  4Ã— Transformer Layers
  Global Pooling: Average all atoms â†’ [128]
  
  Output: Drug embedding [batch_size, 128]
```

**VÃ­ dá»¥**:
```
Aspirin (CCO molecule):
  9 atoms â†’ [9, 78] features
  â†’ 4 Transformer layers (atoms talk to each other)
  â†’ Average pooling â†’ [128] vector (drug representation)
```

#### B. `src/models/protein_encoder.py`
**Logic**: Encode protein sequences

```python
ProteinEncoder:
  Step 1: Embedding
    Amino acid indices [batch, 1000] â†’ [batch, 1000, 128]
  
  Step 2: CNN - Extract Local Motifs
    Conv1D kernel=4:  Capture 4-residue patterns (e.g., HELIX)
    Conv1D kernel=8:  Capture 8-residue patterns (e.g., BETA-SHEET)
    Conv1D kernel=12: Capture 12-residue patterns (e.g., DOMAINS)
    â†’ Concatenate â†’ [batch, 1000, 192] (32+64+96 filters)
  
  Step 3: BiLSTM - Capture Long-Range Dependencies
    Forward LSTM:  Read sequence leftâ†’right
    Backward LSTM: Read sequence rightâ†’left
    â†’ Concatenate â†’ [batch, 1000, 256] (128Ã—2)
  
  Step 4: Global Pooling
    Average over sequence â†’ [batch, 128]
  
  Output: Protein embedding [batch_size, 128]
```

**VÃ­ dá»¥**:
```
Protein "MVKL..." (500 residues):
  â†’ Embedding [500, 128]
  â†’ CNN finds motifs (alpha-helix, beta-sheet)
  â†’ BiLSTM captures long-range interactions
  â†’ Average pooling â†’ [128] vector (protein representation)
```

#### C. `src/models/cross_attention.py`
**Logic**: Learn drug-protein interactions

```python
CrossAttention:
  Input: Drug [batch, 128], Protein [batch, 128]
  
  Step 1: Expand to sequence
    Drug â†’ [batch, 1, 128]     (treat as 1 "token")
    Protein â†’ [batch, 1, 128]
  
  Step 2: Cross-Attention (Drug attends to Protein)
    Q = Drug
    K, V = Protein
    Attention = softmax(QÂ·K^T/âˆšd) Â· V
    â†’ Drug learns "which protein parts matter for binding"
  
  Step 3: Cross-Attention (Protein attends to Drug)
    Q = Protein
    K, V = Drug
    Attention = softmax(QÂ·K^T/âˆšd) Â· V
    â†’ Protein learns "which drug atoms matter for binding"
  
  Step 4: Fusion
    Concatenate [Drug_attended, Protein_attended] â†’ [batch, 256]
    â†’ Linear â†’ [batch, 128]
  
  Output: Fused representation [batch_size, 128]
```

**VÃ­ dá»¥**:
```
Aspirin + Protein interaction:
  Drug [128] + Protein [128]
  â†’ Cross-attention: "Aspirin's OH group binds to Protein's active site"
  â†’ Fused [128] (drug-protein interaction pattern)
```

#### D. `src/models/graphtransdti.py`
**Logic**: Complete end-to-end model

```python
GraphTransDTI.forward():
  Input: Drug graph, Protein sequence [batch, 1000]
  
  # Step 1: Encode Drug
  drug_emb = GraphTransformerEncoder(drug_graph)  # [batch, 128]
  
  # Step 2: Encode Protein
  protein_emb = ProteinEncoder(protein_seq)       # [batch, 128]
  
  # Step 3: Cross-Attention Fusion
  fused = CrossAttention(drug_emb, protein_emb)   # [batch, 128]
  
  # Step 4: Predict Binding Affinity
  x = Linear(fused, 256) â†’ ReLU â†’ Dropout
  x = Linear(x, 128) â†’ ReLU â†’ Dropout
  x = Linear(x, 64) â†’ ReLU â†’ Dropout
  prediction = Linear(x, 1)                       # [batch, 1]
  
  Output: KIBA score prediction (0-17 range)
```

**VÃ­ dá»¥ hoÃ n chá»‰nh**:
```
Input:  Aspirin + Target Protein
Output: 12.5 (KIBA score - binding affinity)

Flow:
  Aspirin SMILES â†’ Graph [9 atoms]
    â†’ Graph Transformer â†’ [128] drug embedding
  
  Protein sequence [500 residues]
    â†’ CNN+BiLSTM â†’ [128] protein embedding
  
  Cross-Attention:
    â†’ Learn "OH group â†” active site" interaction
    â†’ Fused [128]
  
  MLP Predictor:
    â†’ [128] â†’ [256] â†’ [128] â†’ [64] â†’ [1]
    â†’ Output: 12.5 (predicted KIBA score)
```

---

### 5ï¸âƒ£ **TRAINING FILE**

#### `src/train.py`
**Logic**: Main training loop vá»›i early stopping

```python
Trainer.__init__():
  1. Set seed(42) for reproducibility
  2. Initialize model â†’ GPU
  3. Setup optimizer (Adam, lr=0.0001)
  4. Setup scheduler (ReduceLROnPlateau)
  5. Load KIBA train/val dataloaders

Trainer.train_epoch():
  for batch in train_loader:
    # Forward pass
    predictions = model(drug_graph, protein_seq)
    
    # Compute loss
    loss = MSELoss(predictions, labels)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    clip_grad_norm_(parameters, max_norm=1.0)  # Prevent exploding gradients
    optimizer.step()

Trainer.validate():
  with torch.no_grad():
    for batch in val_loader:
      predictions = model(drug_graph, protein_seq)
      loss = MSELoss(predictions, labels)
  
  # Calculate metrics
  RMSE = sqrt(mean((y_true - y_pred)Â²))
  Pearson = correlation(y_true, y_pred)
  CI = concordance_index(y_true, y_pred)

Trainer.train():
  for epoch in 1...100:
    # Train
    train_loss = train_epoch()
    
    # Validate
    val_loss, metrics = validate()
    
    # âœ… EARLY STOPPING LOGIC
    if val_loss < best_val_loss:
      best_val_loss = val_loss
      best_epoch = epoch
      patience_counter = 0
      
      # ğŸ’¾ Save best model
      torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'val_loss': val_loss,
        'val_metrics': metrics
      }, 'checkpoints/GraphTransDTI_KIBA_best.pt')
      
      print(f"âœ“ Saved best model (val_loss: {val_loss:.4f})")
    
    else:
      patience_counter += 1
    
    # ğŸ›‘ Stop if no improvement for 15 epochs
    if patience_counter >= 15:
      print(f"Early stopping at epoch {epoch}")
      print(f"Best epoch: {best_epoch}")
      break
```

**Training Example**:
```
Epoch 1/100
Train Loss: 2.5432
Val Loss: 2.3456 | RMSE: 1.5321 | Pearson: 0.6234 | CI: 0.7123
âœ“ Saved best model (val_loss: 2.3456)

Epoch 2/100
Train Loss: 2.1234
Val Loss: 2.1234 | RMSE: 1.4567 | Pearson: 0.6543 | CI: 0.7345
âœ“ Saved best model (val_loss: 2.1234)

...

Epoch 35/100
Train Loss: 0.8765
Val Loss: 1.0543 | RMSE: 1.0267 | Pearson: 0.8321 | CI: 0.8654
No improvement for 15 epochs
Early stopping triggered at epoch 35
Best epoch: 20 (val_loss: 0.9876)
```

---

### 6ï¸âƒ£ **UTILITY FILES**

#### A. `src/utils/metrics.py`
**Logic**: Evaluation metrics

```python
RMSE (Root Mean Squared Error):
  RMSE = sqrt(mean((y_true - y_pred)Â²))
  â†’ Lower is better (0 = perfect)
  â†’ Measures prediction accuracy

Pearson Correlation:
  r = cov(y_true, y_pred) / (std(y_true) Ã— std(y_pred))
  â†’ Range: -1 to 1 (1 = perfect linear correlation)
  â†’ Measures how well predictions follow true values

Concordance Index (CI):
  CI = P(y_pred_i > y_pred_j | y_true_i > y_true_j)
  â†’ Range: 0 to 1 (1 = perfect ranking)
  â†’ Measures ranking quality (important for drug screening)
```

#### B. `src/utils/seed.py`
**Logic**: Reproducibility

```python
set_seed(42):
  random.seed(42)
  np.random.seed(42)
  torch.manual_seed(42)
  torch.cuda.manual_seed_all(42)
  torch.backends.cudnn.deterministic = True
  
  â†’ Same results every run (important for thesis)
```

#### C. `src/utils/smiles_to_graph.py`
**Logic**: SMILES â†’ PyG Graph conversion

```python
smiles_to_graph("CCO"):
  # Step 1: Parse SMILES
  mol = Chem.MolFromSmiles("CCO")
  
  # Step 2: Extract atom features
  for atom in mol.GetAtoms():
    features = [
      atom.GetAtomicNum(),           # Element (6=C, 8=O)
      atom.GetDegree(),              # Number of bonds
      atom.GetTotalValence(),        # Valence
      atom.GetIsAromatic(),          # Aromatic?
      atom.IsInRing(),               # In ring?
      ...
    ]  # 78 features total
  
  # Step 3: Extract bond features
  for bond in mol.GetBonds():
    edge_features = [
      bond.GetBondType(),            # Single/Double/Triple
      bond.GetIsConjugated(),        # Conjugated?
      bond.IsInRing(),               # In ring?
      ...
    ]  # 12 features total
  
  # Step 4: Create PyG Data
  data = Data(
    x = atom_features,               # [num_atoms, 78]
    edge_index = [[0,1], [1,2], ...],# [2, num_bonds]
    edge_attr = edge_features        # [num_bonds, 12]
  )
```

---

### 7ï¸âƒ£ **EVALUATION FILE**

#### `src/evaluate.py`
**Logic**: Test trÃªn DAVIS dataset (generalization)

```python
Evaluator.__init__():
  1. Load trained model from checkpoint
  2. Load DAVIS test dataloader

Evaluator.evaluate():
  with torch.no_grad():
    for batch in davis_test_loader:
      predictions = model(drug, protein)
      store predictions and labels
  
  # Calculate metrics on DAVIS
  metrics = calculate_metrics(all_labels, all_predictions)
  
  print("DAVIS Test Results:")
  print(f"RMSE: {metrics['rmse']:.4f}")
  print(f"Pearson: {metrics['pearson']:.4f}")
  print(f"CI: {metrics['ci']:.4f}")
```

**Usage**:
```bash
python src/evaluate.py \
  --checkpoint checkpoints/GraphTransDTI_KIBA_best.pt \
  --dataset davis \
  --split test
```

---

## ğŸ”„ TOÃ€N Bá»˜ TRAINING PIPELINE

### **Flowchart**:
```
1. Load Data
   â”œâ”€â”€ data/kiba/ligands_can.txt (2,111 SMILES)
   â”œâ”€â”€ data/kiba/proteins.txt (229 sequences)
   â””â”€â”€ data/kiba/Y (118,254 pairs)

2. Featurization (for each pair)
   â”œâ”€â”€ SMILES â†’ RDKit â†’ Graph [atoms, bonds]
   â”‚   â”œâ”€â”€ Node features: [num_atoms, 78]
   â”‚   â””â”€â”€ Edge features: [num_edges, 12]
   â””â”€â”€ Protein â†’ Token indices â†’ [1000]

3. Model Forward Pass
   â”œâ”€â”€ Drug: Graph â†’ Graph Transformer â†’ [128]
   â”œâ”€â”€ Protein: Sequence â†’ CNN+BiLSTM â†’ [128]
   â”œâ”€â”€ Fusion: Cross-Attention â†’ [128]
   â””â”€â”€ Prediction: MLP â†’ [1] (KIBA score)

4. Training Loop (for each epoch)
   â”œâ”€â”€ Train Phase:
   â”‚   â”œâ”€â”€ Forward pass â†’ predictions
   â”‚   â”œâ”€â”€ Compute loss = MSE(predictions, labels)
   â”‚   â”œâ”€â”€ Backward pass â†’ gradients
   â”‚   â””â”€â”€ Update weights
   â”‚
   â””â”€â”€ Validation Phase:
       â”œâ”€â”€ Forward pass (no gradients)
       â”œâ”€â”€ Compute metrics (RMSE, Pearson, CI)
       â””â”€â”€ Check early stopping:
           â”œâ”€â”€ If val_loss improved â†’ Save model
           â””â”€â”€ If no improvement for 15 epochs â†’ STOP

5. Output
   â”œâ”€â”€ checkpoints/GraphTransDTI_KIBA_best.pt (best model)
   â”œâ”€â”€ training_history.pkl (loss curves)
   â””â”€â”€ logs/ (tensorboard logs)
```

---

## ğŸ“Š Káº¾T QUáº¢ MONG Äá»¢I

### Training (KIBA):
- **Best Epoch**: ~30-50 (with early stopping)
- **Training Time**: 2-3 giá» trÃªn RTX 3050
- **Best Val Loss**: ~0.8-1.0
- **Val Metrics**:
  - RMSE: 0.9-1.1 (â†“ cÃ ng tháº¥p cÃ ng tá»‘t)
  - Pearson: 0.82-0.88 (â†‘ cÃ ng cao cÃ ng tá»‘t)
  - CI: 0.85-0.90 (â†‘ cÃ ng cao cÃ ng tá»‘t)

### Generalization (DAVIS):
- **Test Metrics**:
  - RMSE: 1.0-1.2
  - Pearson: 0.78-0.84
  - CI: 0.82-0.87

### So sÃ¡nh vá»›i Baselines:
```
Model            | RMSE (KIBA) | Pearson | CI
-----------------+-------------+---------+------
DeepDTA          | 1.15        | 0.78    | 0.82
GraphDTA         | 1.05        | 0.83    | 0.85
GraphTransDTI    | 0.95        | 0.87    | 0.88  â† Má»¤C TIÃŠU
(10% improvement)
```

---

## ğŸš€ CÃCH CHáº Y TRAINING

### Quick Test (5 epochs):
```bash
# Edit config.yaml: num_epochs: 5
python src/train.py
```

### Full Training (100 epochs vá»›i early stopping):
```bash
python src/train.py
# Sáº½ cháº¡y 2-3 giá»
# Tá»± Ä‘á»™ng stop náº¿u khÃ´ng improve sau 15 epochs
```

### Evaluate on DAVIS:
```bash
python src/evaluate.py \
  --checkpoint checkpoints/GraphTransDTI_KIBA_best.pt \
  --dataset davis \
  --split test
```

---

## ğŸ’¾ OUTPUT FILES

Sau khi training xong:
```
checkpoints/
â””â”€â”€ GraphTransDTI_KIBA_best.pt         # Model tá»‘t nháº¥t (dÃ¹ng cho thesis)
    â”œâ”€â”€ model_state_dict               # Model weights
    â”œâ”€â”€ epoch                          # Epoch nÃ o Ä‘áº¡t best
    â”œâ”€â”€ val_loss                       # Val loss tá»‘t nháº¥t
    â””â”€â”€ val_metrics                    # RMSE, Pearson, CI

logs/
â””â”€â”€ training.log                       # Chi tiáº¿t tá»«ng epoch

training_history.pkl                   # Loss curves (dÃ¹ng Ä‘á»ƒ váº½ Ä‘á»“ thá»‹)
```

---

## ğŸ“ˆ Váº¼ Äá»’ THá»Š

Sá»­ dá»¥ng `src/plot_results.py`:
```python
import pickle
import matplotlib.pyplot as plt

# Load history
with open('checkpoints/GraphTransDTI_KIBA_history.pkl', 'rb') as f:
    history = pickle.load(f)

# Plot loss curves
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss_curve.png')
```

---

## âœ… CHECKLIST TRÆ¯á»šC KHI TRAIN

- [x] GPU setup (CUDA available)
- [x] PyTorch 2.5.1+cu121 installed
- [x] Data downloaded (KIBA: 118K pairs)
- [x] Dataloaders verified
- [x] Model architecture complete
- [x] Config.yaml ready
- [x] Early stopping implemented
- [ ] Ready to start training!

---

## ğŸ¯ TÃ“M Táº®T

**CÃC FILE CHÃNH**:
1. `config.yaml` - Hyperparameters
2. `src/train.py` - Training script âœ… Early stopping
3. `src/dataloader/` - Load & featurize data
4. `src/models/` - GraphTransDTI architecture
5. `src/utils/` - Metrics, seed, SMILES converter
6. `src/evaluate.py` - Test on DAVIS

**LOGIC**:
- SMILES â†’ Graph (78-dim atoms, 12-dim bonds)
- Protein â†’ Tokens (26 amino acids, pad to 1000)
- Graph Transformer (4 layers) â†’ Drug [128]
- CNN+BiLSTM â†’ Protein [128]
- Cross-Attention â†’ Fusion [128]
- MLP â†’ Binding affinity [1]
- Early stopping: LÆ°u model tá»‘t nháº¥t, dá»«ng náº¿u 15 epochs khÃ´ng improve

**EXPECTED TIME**: 2-3 giá» trÃªn RTX 3050

**READY TO TRAIN!** ğŸš€
