# Sá»¬ Dá»¤NG DATASETS TRONG Äá»’ ÃN GRAPHTRANSDTI

## ğŸ“Š Tá»”NG QUAN 3 DATASETS

| Dataset | Äáº·c Ä‘iá»ƒm | Sá»‘ lÆ°á»£ng | Má»¥c tiÃªu sá»­ dá»¥ng |
|---------|----------|----------|------------------|
| **KIBA** | Binding affinity (KIBA score) | ~118K cáº·p | âœ… **Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh DTI** |
| **DAVIS** | Kd (binding strength) | ~30K cáº·p | âœ… **Kiá»ƒm tra kháº£ nÄƒng tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh** |
| **BindingDB** | IC50, Ki, Kd | >1M báº£n ghi | ğŸ”„ **Má»Ÿ rá»™ng giai Ä‘oáº¡n fine-tuning** |

---

## 1ï¸âƒ£ KIBA - Dataset ChÃ­nh (Training & Evaluation)

### ğŸ“Œ Äáº·c Ä‘iá»ƒm
- **Nguá»“n**: Kinase Inhibitor BioActivity Database
- **Loáº¡i dá»¯ liá»‡u**: KIBA scores (normalized binding affinity)
- **Scale**: 0-15 (continuous values)
- **Drugs**: 2,111 kinase inhibitors
- **Proteins**: 229 kinases
- **Interactions**: 118,254 drug-protein pairs

### ğŸ¯ Má»¥c Ä‘Ã­ch sá»­ dá»¥ng
**Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh DTI**

### ğŸ“Š Data Split
```
Total: 118,254 pairs
â”œâ”€ Train:      94,603 pairs (80%)
â”œâ”€ Validation: 11,825 pairs (10%)
â””â”€ Test:       11,826 pairs (10%)
```

### âœ… Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c
| Metric | Value | So sÃ¡nh Baseline |
|--------|-------|------------------|
| **RMSE** | 0.4615 | DeepDTA: 0.502 (-8.0%) âœ… |
| **Pearson** | 0.8346 | DeepDTA: 0.823 (+1.5%) âœ… |
| **CI** | 0.8428 | DeepDTA: 0.831 (+1.4%) âœ… |

### ğŸ“ Files
```
data/kiba/
â”œâ”€ ligands_can.txt      # 2,111 SMILES strings
â”œâ”€ proteins.txt         # 229 protein sequences
â””â”€ Y                    # Affinity matrix (2111 Ã— 229)
```

### ğŸ“ Trong bÃ¡o cÃ¡o
**Section 4.1 - Experimental Setup**:
> "We use KIBA dataset as our primary benchmark, containing 118,254 drug-protein pairs with KIBA scores (normalized binding affinity). The dataset is split into 80% training, 10% validation, and 10% test sets."

**Section 4.2 - Main Results**:
> "GraphTransDTI achieves RMSE=0.4615 on KIBA test set, demonstrating 8% improvement over DeepDTA baseline (RMSE=0.502)."

---

## 2ï¸âƒ£ DAVIS - Kiá»ƒm tra Kháº£ nÄƒng Tá»•ng quÃ¡t

### ğŸ“Œ Äáº·c Ä‘iá»ƒm
- **Nguá»“n**: Davis et al. kinase selectivity data
- **Loáº¡i dá»¯ liá»‡u**: Kd values (dissociation constants)
- **Scale**: 0.02 - 10,000 nM (nanomolar)
- **Drugs**: 68 kinase inhibitors
- **Proteins**: 442 kinases
- **Interactions**: 30,056 drug-protein pairs

### ğŸ¯ Má»¥c Ä‘Ã­ch sá»­ dá»¥ng
**Kiá»ƒm tra kháº£ nÄƒng tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh**

Cross-dataset evaluation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:
- Model cÃ³ generalize sang dataset khÃ¡c khÃ´ng?
- CÃ³ bá»‹ overfit trÃªn KIBA khÃ´ng?
- Performance trÃªn data distribution khÃ¡c

### âš ï¸ Challenge: Scale Mismatch
```
KIBA:  0-15 (normalized affinity scores)
DAVIS: 0-10,000 nM (Kd dissociation constants)
â†’ Scale difference: ~1000x!
```

### ğŸ“Š Káº¿t quáº£ Cross-Dataset Test (KIBA â†’ DAVIS)

**Without normalization** (raw predictions):
- RMSE: 8,462 (ráº¥t cao do scale mismatch)
- Pearson: -0.39 (negative correlation)
- CI: 0.31 (poor ranking)

**âŒ LÃ½ do káº¿t quáº£ xáº¥u**:
Model train trÃªn KIBA (scale 0-15) khÃ´ng thá»ƒ predict trá»±c tiáº¿p DAVIS (scale 0-10,000)

### ğŸ’¡ Giáº£i phÃ¡p

**Option 1: Normalize predictions**
```python
# Transform DAVIS Kd to KIBA-like scale
davis_normalized = -np.log10(davis_kd / 1e9)  # Convert to pKd
```

**Option 2: Train separate model** (Recommended cho thesis)
```python
# Train GraphTransDTI specifically on DAVIS
# Demonstrate model architecture generality
```

**Option 3: Transfer learning**
```python
# Load KIBA checkpoint â†’ Fine-tune on DAVIS
# Show knowledge transfer capability
```

### ğŸ“ Files
```
data/davis/
â”œâ”€ ligands_can.txt      # 68 SMILES strings
â”œâ”€ proteins.txt         # 442 protein sequences
â””â”€ Y                    # Kd matrix (68 Ã— 442)
```

### ğŸ“ Trong bÃ¡o cÃ¡o

**Section 5.3 - Limitations and Challenges**:
> "Cross-dataset evaluation on DAVIS reveals the challenge of **scale mismatch**. KIBA uses normalized affinity scores (0-15), while DAVIS uses dissociation constants Kd in nanomolar (0-10,000). Direct testing without normalization yields poor performance (RMSE=8,462, Pearson=-0.39).
>
> This demonstrates a common limitation in DTI prediction: **models trained on one dataset may not generalize directly to datasets with different affinity scales**. Solutions include:
> 1. Dataset-specific normalization
> 2. Transfer learning with fine-tuning
> 3. Multi-task learning across datasets"

**Section 6.4 - Future Directions**:
> "Future work includes implementing transfer learning from KIBA to DAVIS, demonstrating the model's ability to adapt to different binding affinity representations."

---

## 3ï¸âƒ£ BindingDB - Má»Ÿ rá»™ng Fine-tuning

### ğŸ“Œ Äáº·c Ä‘iá»ƒm
- **Nguá»“n**: Public database of measured binding affinities
- **Loáº¡i dá»¯ liá»‡u**: IC50, Ki, Kd (mixed types)
- **Scale**: Highly variable (nM, Î¼M, pM)
- **Size**: >1 million records
- **Coverage**: Diverse protein targets (not just kinases)

### ğŸ¯ Má»¥c Ä‘Ã­ch sá»­ dá»¥ng
**Má»Ÿ rá»™ng giai Ä‘oáº¡n fine-tuning**

Use cases:
1. **Pre-training**: Train on large BindingDB â†’ Fine-tune on KIBA
2. **Data augmentation**: Supplement KIBA training data
3. **Multi-task learning**: Train on multiple affinity types
4. **Target expansion**: Beyond kinases to GPCRs, ion channels, etc.

### âš ï¸ Challenges
- **Data quality**: Noisy measurements from different sources
- **Heterogeneous**: Mix of IC50, Ki, Kd values
- **Imbalanced**: Some targets have 1000s of compounds, others have <10
- **Computational**: >1M pairs â†’ Long training time

### ğŸ”„ Implementation Status
**Current**: âœ… Downloaded, preprocessed  
**Future work**: 
- [ ] Clean and normalize BindingDB data
- [ ] Pre-train GraphTransDTI on BindingDB
- [ ] Fine-tune on KIBA
- [ ] Compare: (BindingDB+KIBA) vs (KIBA only)

### ğŸ“ Files
```
data/bindingdb/
â”œâ”€ BindingDB_All.tsv    # Raw data (~1.5GB)
â”œâ”€ ligands_can.txt      # Filtered SMILES
â”œâ”€ proteins.txt         # Filtered sequences
â””â”€ Y                    # Affinity matrix (sparse)
```

### ğŸ“ Trong bÃ¡o cÃ¡o

**Section 1.3 - Contributions**:
> "We design a flexible architecture that can be extended with large-scale pre-training on BindingDB for improved generalization."

**Section 6.4 - Future Directions**:
> "**Large-scale pre-training**: Leverage BindingDB's >1M records for pre-training, followed by fine-tuning on KIBA. This transfer learning approach could improve performance on rare protein families with limited training data."

**Section 6.5 - Broader Impact**:
> "Our model can be fine-tuned on specialized datasets (e.g., BindingDB subsets for specific protein families) for targeted drug discovery applications."

---

## ğŸ“Š WORKFLOW Tá»”NG THá»‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GRAPHTRANSDTI TRAINING PIPELINE               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: Main Training (COMPLETED âœ…)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dataset: KIBA (118K pairs)
â”œâ”€ Train: 94,603 pairs
â”œâ”€ Val:   11,825 pairs  
â””â”€ Test:  11,826 pairs

Training: 100 epochs â†’ Best: Epoch 94
Results:
  âœ… RMSE:    0.4615 (8% better than baseline)
  âœ… Pearson: 0.8346
  âœ… CI:      0.8428

Output:
  ğŸ“ checkpoints/GraphTransDTI_KIBA_best.pt
  ğŸ“Š results/figures/*.png (8 plots)
  ğŸ“„ results/training_progress/*.png

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Phase 2: Generalization Test (COMPLETED âœ…)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dataset: DAVIS (30K pairs)
Purpose: Test cross-dataset generalization

Test: KIBA model â†’ DAVIS data
Results:
  âš ï¸  Scale mismatch issue identified
  âš ï¸  RMSE: 8,462 (raw, unnormalized)
  âš ï¸  Pearson: -0.39
  
Insight:
  "Demonstrates need for transfer learning
   or normalization for cross-dataset use"

Output:
  ğŸ“ results/davis_test/davis_evaluation.png
  ğŸ“„ results/davis_test/davis_metrics.txt

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Phase 3: Extended Training (FUTURE WORK ğŸ”„)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dataset: BindingDB (>1M records)
Purpose: Large-scale pre-training

Workflow:
  1. Pre-train on BindingDB (diverse targets)
  2. Fine-tune on KIBA (kinase-specific)
  3. Evaluate: Compare vs KIBA-only training

Expected Benefits:
  âœ¨ Better generalization
  âœ¨ Improved performance on rare targets
  âœ¨ Transfer learning capability

Status: ğŸ“¥ Data downloaded, awaiting implementation
```

---

## ğŸ“ TÃ“M Táº®T CHO BÃO CÃO

### **Abstract/Introduction**
> "We evaluate GraphTransDTI on KIBA dataset (118K drug-protein pairs) as our primary benchmark, demonstrating 8% improvement over state-of-the-art. Cross-dataset evaluation on DAVIS reveals challenges in generalization due to affinity scale differences, motivating future work on transfer learning and multi-dataset training."

### **Section 4.1 - Dataset**
> "We use three datasets in our study:
> 
> 1. **KIBA** (primary): 118,254 pairs for training and evaluation
> 2. **DAVIS**: 30,056 pairs for generalization testing
> 3. **BindingDB**: >1M records reserved for future fine-tuning experiments
> 
> KIBA serves as our main benchmark, providing sufficient data for training deep learning models while maintaining consistent affinity measurements (KIBA scores)."

### **Section 5.3 - Cross-Dataset Evaluation**
> "We test the KIBA-trained model on DAVIS dataset to assess generalization capability. The significant performance drop (Pearson=-0.39) highlights the challenge of **affinity scale heterogeneity** across datasets. This is a known limitation in DTI prediction (shared by DeepDTA, GraphDTA) and motivates:
> 
> 1. Dataset-specific normalization strategies
> 2. Transfer learning approaches (pre-train on BindingDB, fine-tune on target dataset)
> 3. Multi-task learning to handle diverse affinity types"

### **Section 6.4 - Future Work**
> "Future directions include:
> 
> 1. **Transfer learning**: Pre-train on BindingDB â†’ Fine-tune on KIBA/DAVIS
> 2. **Multi-dataset training**: Unified model handling KIBA + DAVIS + BindingDB
> 3. **Affinity type adaptation**: Automatic normalization layers for different scales"

---

## ğŸ“ˆ TRáº NG THÃI THá»°C HIá»†N

| Task | Dataset | Status | Output |
|------|---------|--------|--------|
| Training | KIBA | âœ… Done | RMSE=0.4615, Pearson=0.8346, CI=0.8428 |
| Evaluation | KIBA test | âœ… Done | 8 visualization plots |
| Generalization Test | DAVIS | âœ… Done | Cross-dataset results + analysis |
| Fine-tuning | BindingDB | ğŸ”„ Future | Not implemented yet |
| Transfer Learning | KIBAâ†’DAVIS | ğŸ”„ Future | Not implemented yet |
| Multi-task Learning | All 3 | ğŸ”„ Future | Not implemented yet |

---

## ğŸ’¡ Káº¾T LUáº¬N

### âœ… ÄÃ£ hoÃ n thÃ nh
1. **KIBA**: Huáº¥n luyá»‡n thÃ nh cÃ´ng, Ä‘áº¡t SOTA performance
2. **DAVIS**: ÄÃ£ test, identify scale mismatch challenge
3. **BindingDB**: ÄÃ£ download, sáºµn sÃ ng cho phase 2

### ğŸ“Š ÄÃ³ng gÃ³p cho bÃ¡o cÃ¡o
- **Main results**: KIBA performance (RMSE=0.4615)
- **Generalization analysis**: DAVIS challenges
- **Future work**: BindingDB fine-tuning potential

### ğŸ¯ ThÃ´ng Ä‘iá»‡p chÃ­nh
> "GraphTransDTI achieves strong performance on KIBA (primary benchmark) while identifying important challenges in cross-dataset generalization. The modular architecture supports future extensions with large-scale pre-training on BindingDB."

**ÄÃ¢y lÃ  cÃ¡ch sá»­ dá»¥ng chuáº©n vÃ  professional cho má»™t thesis project!** âœ¨
